{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SgNZTjrhcHa0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoavGutmanUCSD/pa3-cse156/blob/main/PA3_CS156_SP24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "# CSE 156:  NLP UCSD, Programming Assignment 3\n",
        "## Text Decoding From GPT-2 using Beam Search (40 points)\n",
        "### <font color='blue'> Due: Friday, May 31, at  10pm </font>\n",
        "\n",
        "###### IMPORTANT: After copying this notebook to your Google Drive, paste a link to it below. To get a publicly-accessible link, click the *Share* button at the top right, then click \"Get shareable link\" and copy the link.\n",
        "#### <font color=\"red\">Link: paste your link here:  </font>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Notes:**\n",
        "\n",
        "Make sure to save the notebook as you go along.\n",
        "\n",
        "Submission instructions are located at the bottom of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23zfO_ALKeB"
      },
      "source": [
        "# Part 0: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N25dvF4jvYoy"
      },
      "source": [
        "## Adding a hardware accelerator\n",
        "Go to the menu and add a GPU as follows:\n",
        "\n",
        "`Edit > Notebook Settings > Hardware accelerator > (GPU)`\n",
        "\n",
        "Run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edOh9ooiIW1B"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvH7xx9LnMC"
      },
      "source": [
        "## Installing Hugging Face's Transformers and Additional Libraries\n",
        "We will use Hugging Face's Transformers (https://github.com/huggingface/transformers).\n",
        "\n",
        "Run the following cell to install Hugging Face's Transformers library and some other useful tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtqS2e5fxpqa"
      },
      "source": [
        "!pip install -q transformers==4.17.0  rich[jupyter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Beam Search\n",
        "We are going to explore decoding from a pretrained GPT-2 model using beam search. Run the below cell to set up some beam search utilities."
      ],
      "metadata": {
        "id": "sa5Q_y4huAUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Beam Search\n",
        "\n",
        "def init_beam_search(model, input_ids, num_beams):\n",
        "    assert len(input_ids.shape) == 2\n",
        "    beam_scores = torch.zeros(num_beams, dtype=torch.float32, device=model.device)\n",
        "    beam_scores[1:] = -1e9 # Break ties in first round.\n",
        "    new_input_ids = input_ids.repeat_interleave(num_beams, dim=0).to(model.device)\n",
        "    return new_input_ids, beam_scores\n",
        "\n",
        "\n",
        "def run_beam_search_(model, tokenizer, input_text, num_beams=5, num_decode_steps=10, score_processors=[], to_cpu=True):\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    input_ids, beam_scores = init_beam_search(model, input_ids, num_beams)\n",
        "\n",
        "    token_scores = beam_scores.clone().view(num_beams, 1)\n",
        "\n",
        "    model_kwargs = {}\n",
        "    for i in range(num_decode_steps):\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "        outputs = model(**model_inputs, return_dict=True)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "        vocab_size = next_token_logits.shape[-1]\n",
        "        this_token_scores = torch.log_softmax(next_token_logits, -1)\n",
        "\n",
        "        # Process token scores.\n",
        "        processed_token_scores = this_token_scores\n",
        "        for processor in score_processors:\n",
        "            processed_token_scores = processor(input_ids, processed_token_scores)\n",
        "\n",
        "        # Update beam scores.\n",
        "        next_token_scores = processed_token_scores + beam_scores.unsqueeze(-1)\n",
        "\n",
        "        # Reshape for beam-search.\n",
        "        next_token_scores = next_token_scores.view(num_beams * vocab_size)\n",
        "\n",
        "        # Find top-scoring beams.\n",
        "        next_token_scores, next_tokens = torch.topk(\n",
        "            next_token_scores, num_beams, dim=0, largest=True, sorted=True\n",
        "        )\n",
        "\n",
        "        # Transform tokens since we reshaped earlier.\n",
        "        next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\") # This is equivalent to `next_tokens // vocab_size`\n",
        "        next_tokens = next_tokens % vocab_size\n",
        "\n",
        "        # Update tokens.\n",
        "        input_ids = torch.cat([input_ids[next_indices, :], next_tokens.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # Update beam scores.\n",
        "        beam_scores = next_token_scores\n",
        "\n",
        "        # Update token scores.\n",
        "\n",
        "        # UNCOMMENT: To use original scores instead.\n",
        "        # token_scores = torch.cat([token_scores[next_indices, :], this_token_scores[next_indices, next_tokens].unsqueeze(-1)], dim=-1)\n",
        "        token_scores = torch.cat([token_scores[next_indices, :], processed_token_scores[next_indices, next_tokens].unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # Update hidden state.\n",
        "        model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=False)\n",
        "        model_kwargs[\"past\"] = model._reorder_cache(model_kwargs[\"past\"], next_indices)\n",
        "\n",
        "    def transfer(x):\n",
        "      return x.cpu() if to_cpu else x\n",
        "\n",
        "    return {\n",
        "        \"output_ids\": transfer(input_ids),\n",
        "        \"beam_scores\": transfer(beam_scores),\n",
        "        \"token_scores\": transfer(token_scores)\n",
        "    }\n",
        "\n",
        "\n",
        "def run_beam_search(*args, **kwargs):\n",
        "    with torch.inference_mode():\n",
        "        return run_beam_search_(*args, **kwargs)\n",
        "\n",
        "\n",
        "# Add support for colored printing and plotting.\n",
        "\n",
        "from rich import print as rich_print\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "RICH_x = np.linspace(0.0, 1.0, 50)\n",
        "RICH_rgb = (matplotlib.colormaps.get_cmap(plt.get_cmap('RdYlBu'))(RICH_x)[:, :3] * 255).astype(np.int32)[range(5, 45, 5)]\n",
        "\n",
        "\n",
        "def print_with_probs(words, probs, prefix=None):\n",
        "  def fmt(x, p, is_first=False):\n",
        "    ix = int(p * RICH_rgb.shape[0])\n",
        "    r, g, b = RICH_rgb[ix]\n",
        "    if is_first:\n",
        "      return f'[bold rgb(0,0,0) on rgb({r},{g},{b})]{x}'\n",
        "    else:\n",
        "      return f'[bold rgb(0,0,0) on rgb({r},{g},{b})] {x}'\n",
        "  output = []\n",
        "  if prefix is not None:\n",
        "    output.append(prefix)\n",
        "  for i, (x, p) in enumerate(zip(words, probs)):\n",
        "    output.append(fmt(x, p, is_first=i == 0))\n",
        "  rich_print(''.join(output))\n",
        "\n",
        "# DEMO\n",
        "\n",
        "# Show range of colors.\n",
        "\n",
        "for i in range(RICH_rgb.shape[0]):\n",
        "  r, g, b = RICH_rgb[i]\n",
        "  rich_print(f'[bold rgb(0,0,0) on rgb({r},{g},{b})]hello world rgb({r},{g},{b})')\n",
        "\n",
        "# Example with words and probabilities.\n",
        "\n",
        "words = ['the', 'brown', 'fox']\n",
        "probs = [0.14, 0.83, 0.5]\n",
        "print_with_probs(words, probs)"
      ],
      "metadata": {
        "id": "bgUbrB6ht_ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.1 (5 points)\n",
        "\n",
        "Run the cell below. It produces a sequence of tokens using beam search and the provided prefix."
      ],
      "metadata": {
        "id": "lbieUEZViMJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_beams = 5\n",
        "num_decode_steps = 10\n",
        "input_text = 'The brown fox jumps'\n",
        "\n",
        "beam_output = run_beam_search(model, tokenizer, input_text, num_beams=num_beams, num_decode_steps=num_decode_steps)\n",
        "for i, tokens in enumerate(beam_output['output_ids']):\n",
        "    score = beam_output['beam_scores'][i]\n",
        "    print(i, round(score.item() / tokens.shape[-1], 3), tokenizer.decode(tokens, skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "WccupVHjymFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get you more acquainted with the code, let's do a simple exercise first. Write your own code in the cell below to generate 3 tokens with a beam size of 4, and then print out the **third most probable** output sequence found during the search. Use the same prefix as above."
      ],
      "metadata": {
        "id": "t6zN9dbNLBl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'The brown fox jumps'\n",
        "\n",
        "# WRITE YOUR CODE HERE!"
      ],
      "metadata": {
        "id": "STizG6yiLqIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.2 (5 points)\n",
        "\n",
        "Run the cell below to visualize the probabilities the model assigns for each generated word when using beam search with beam size 1 (i.e., greedy decoding)."
      ],
      "metadata": {
        "id": "15aHJGr8p0mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'The brown fox jumps'\n",
        "beam_output = run_beam_search(model, tokenizer, input_text, num_beams=1, num_decode_steps=20)\n",
        "probs = beam_output['token_scores'][0, 1:].exp()\n",
        "output_subwords = [tokenizer.decode(tok, skip_special_tokens=True) for tok in beam_output['output_ids'][0]]\n",
        "\n",
        "print('Visualizeation with plot:')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(range(len(probs)), probs)\n",
        "ax.set_xticks(range(len(probs)))\n",
        "ax.set_xticklabels(output_subwords[-len(probs):], rotation = 45)\n",
        "plt.xlabel('word')\n",
        "plt.ylabel('prob')\n",
        "plt.show()\n",
        "\n",
        "print('Visualization with colored text (red for lower probability, and blue for higher):')\n",
        "\n",
        "print_with_probs(output_subwords[-len(probs):], probs, ' '.join(output_subwords[:-len(probs)]))"
      ],
      "metadata": {
        "id": "lDXNNlktqIUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why does the model assign higher probability to tokens generated later than to tokens generated earlier?"
      ],
      "metadata": {
        "id": "KZq6Zhw0MKuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">Write your answer here </font>\n",
        "\n"
      ],
      "metadata": {
        "id": "lY8G4tJMMGl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Run the cell below to visualize the word probabilities when using different beam sizes."
      ],
      "metadata": {
        "id": "Boog4GEBsBCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'Once upon a time, in a barn near a farm house,'\n",
        "num_decode_steps = 20\n",
        "model.cuda()\n",
        "\n",
        "beam_size_list = [1, 2, 3, 4, 5]\n",
        "output_list = []\n",
        "probs_list = []\n",
        "for bm in beam_size_list:\n",
        "  beam_output = run_beam_search(model, tokenizer, input_text, num_beams=bm, num_decode_steps=num_decode_steps)\n",
        "  output_list.append(beam_output)\n",
        "  probs = beam_output['token_scores'][0, 1:].exp()\n",
        "  probs_list.append((bm, probs))\n",
        "\n",
        "print('Visualization with plot:')\n",
        "fig, ax = plt.subplots()\n",
        "for bm, probs in probs_list:\n",
        "  plt.plot(range(len(probs)), probs, label=f'beam size = {bm}')\n",
        "plt.xlabel('decode step')\n",
        "plt.ylabel('prob')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "print('Model predictions:')\n",
        "for bm, beam_output in zip(beam_size_list, output_list):\n",
        "  tokens = beam_output['output_ids'][0]\n",
        "  print(bm, beam_output['beam_scores'][0].item() / tokens.shape[-1], tokenizer.decode(tokens, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "heX5mLxe-s-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.3 (10 points)\n",
        "\n",
        "Beam search often results in repetition in the predicted tokens. In the following cell we pass a score processor called `WordBlock` to `run_beam_search`. At each time step, it reduces the probability for any previously seen word so that it is not generated again.\n",
        "\n",
        "Run the cell to see how the output of beam search changes with and without using `WordBlock`."
      ],
      "metadata": {
        "id": "uX4P6tTmtJ3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class WordBlock:\n",
        "    def __call__(self, input_ids, scores):\n",
        "        for batch_idx in range(input_ids.shape[0]):\n",
        "            for x in input_ids[batch_idx].tolist():\n",
        "                scores[batch_idx, x] = -1e9\n",
        "        return scores\n",
        "\n",
        "input_text = 'Once upon a time, in a barn near a farm house,'\n",
        "num_beams = 1\n",
        "\n",
        "print('Beam Search')\n",
        "beam_output = run_beam_search(model, tokenizer, input_text, num_beams=num_beams, num_decode_steps=40, score_processors=[])\n",
        "print(tokenizer.decode(beam_output['output_ids'][0], skip_special_tokens=True))\n",
        "\n",
        "print('Beam Search w/ Word Block')\n",
        "beam_output = run_beam_search(model, tokenizer, input_text, num_beams=num_beams, num_decode_steps=40, score_processors=[WordBlock()])\n",
        "print(tokenizer.decode(beam_output['output_ids'][0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "yTu_HemeLquc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is `WordBlock` a practical way to prevent repetition in beam search? What (if anything) could go wrong when using `WordBlock`?"
      ],
      "metadata": {
        "id": "BMJ8KVrZSTQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"red\">Write your answer here </font>\n",
        "\n"
      ],
      "metadata": {
        "id": "WT0Jv2SBTinm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1.4 (20 points)\n",
        "\n",
        "Use the previous `WordBlock` example to write a new score processor called `BeamBlock`. Instead of uni-grams, your implementation should prevent tri-grams from appearing more than once in the sequence.\n",
        "\n",
        "Note: This technique is called \"beam blocking\" and is described [here](https://arxiv.org/pdf/1705.04304.pdf) (section 2.5). Also, for this assignment you do not need to re-normalize your output distribution after masking values, although typically re-normalization is done.\n",
        "\n",
        "Write your code in the indicated section in the below cell."
      ],
      "metadata": {
        "id": "U4937n1-t6l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "class BeamBlock:\n",
        "    def __call__(self, input_ids, scores):\n",
        "        for batch_idx in range(input_ids.shape[0]):\n",
        "            # WRITE YOUR CODE HERE!\n",
        "            pass\n",
        "        return scores\n",
        "\n",
        "input_text = 'Once upon a time, in a barn near a farm house,'\n",
        "num_beams = 1\n",
        "\n",
        "print('Beam Search')\n",
        "beam_output = run_beam_search(model, tokenizer, input_text, num_beams=num_beams, num_decode_steps=40, score_processors=[])\n",
        "print(tokenizer.decode(beam_output['output_ids'][0], skip_special_tokens=True))\n",
        "\n",
        "print('Beam Search w/ Beam Block')\n",
        "beam_output = run_beam_search(model, tokenizer, input_text, num_beams=num_beams, num_decode_steps=40, score_processors=[BeamBlock()])\n",
        "print(tokenizer.decode(beam_output['output_ids'][0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "PTQUGTWgtuBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# <font color=\"blue\"> Submission Instructions</font>\n",
        "\n",
        "1. Click the Save button at the top of the Jupyter Notebook.\n",
        "2. Select Edit -> Clear All Outputs. This will clear all the outputs from all cells (but will keep the content of all cells).\n",
        "3. Select Runtime -> Run All. This will run all the cells in order, and will take several minutes.\n",
        "4. Once you've rerun everything, save a PDF version of your notebook. <font color='blue'> Make sure all your solutions especially the coding parts are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
        "5. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
        "6. Submit your PDF on Gradescope.\n",
        "\n",
        "\n",
        "\n",
        "#### <font color=\"blue\"> Acknowledgements</font>\n",
        "This assignment is based on an assignment developed by Mohit Iyyer"
      ],
      "metadata": {
        "id": "72bCEqcNQWyw"
      }
    }
  ]
}